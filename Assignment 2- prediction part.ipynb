{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict  # Dictionaries with default values\n",
    "from math import log10\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from nltk import bigrams\n",
    "from sklearn import linear_model\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "path = \"renttherunway_final_data.json\"\n",
    "f = open(path, \"r\")\n",
    "\n",
    "dataset_ = []\n",
    "for i in f:\n",
    "    dataset_.append(json.loads(i))\n",
    "\n",
    "# build dataset dataframe\n",
    "dataset = pd.DataFrame()\n",
    "dataset = dataset.append(dataset_, ignore_index=True)\n",
    "dataset = dataset.drop([\"bust size\", \"weight\", \"body type\", \"height\", \"size\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some review data do not have age features, we calculate the average age from the remaining data\n",
    "age = []\n",
    "for i in dataset_:\n",
    "    if \"age\" in i.keys():\n",
    "        age.append(int(i[\"age\"]))\n",
    "avgAge = sum(age) / len(age)\n",
    "\n",
    "dataset[[\"age\"]] = dataset[[\"age\"]].fillna(avgAge)\n",
    "dataset[[\"age\"]] = dataset[[\"age\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some data does not have rating info (82), we remove these data\n",
    "for i in range(len(dataset[\"rating\"])):\n",
    "    if dataset[\"rating\"][i] == None:\n",
    "        dataset = dataset.drop(i)\n",
    "dataset[[\"rating\"]] = dataset[[\"rating\"]].astype(int)\n",
    "dataset[\"rating\"] = dataset[\"rating\"].map(lambda x: x / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# segment data as positive and negative\n",
    "def partition(x):\n",
    "    if x <= 3:\n",
    "        return \"negative\"\n",
    "    return \"positive\"\n",
    "\n",
    "\n",
    "# actualScore = filtered_data['Score']\n",
    "# positiveNegative = actualScore.map(partition)\n",
    "dataset[\"P/N\"] = dataset[\"rating\"].map(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle the data and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "dataset = shuffle(dataset)\n",
    "random.seed(1234)\n",
    "\n",
    "Ntrain, Nvalid, Ntest = (\n",
    "    round(len(dataset) * 0.8),\n",
    "    round(len(dataset) * 0.1),\n",
    "    round(len(dataset) * 0.1),\n",
    ")\n",
    "# split the data\n",
    "data_train = dataset[:Ntrain]\n",
    "data_valid = dataset[Ntrain : Nvalid + Ntrain]\n",
    "data_test = dataset[Nvalid + Ntrain :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive classifier \n",
    "f (t) = Î± The main idea for this classifier is that we generates predictions by computing averages for each user, or return the global average if we've never seen the user before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global average and each user average in training data\n",
    "y_train = data_train[[\"rating\"]]\n",
    "globalAverage = y_train.mean()\n",
    "\n",
    "UserAvg = data_train.groupby(\"user_id\")[\"rating\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mse function\n",
    "def MSE(pred, data):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(pred, data)]\n",
    "    mse = sum(differences) / len(differences)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define predictor\n",
    "def predict(user_id):\n",
    "    if user_id in UserAvg.keys():\n",
    "        pred = UserAvg[user_id]\n",
    "    else:\n",
    "        pred = globalAverage\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set is rating    0.614557\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# test on validation set\n",
    "y_valid = data_valid[\"rating\"]\n",
    "y_pred_valid = [predict(i) for i in data_valid[\"user_id\"]]\n",
    "MSE_valid = MSE(y_pred_valid, y_valid)\n",
    "print(\"MSE on validation set is\", str(MSE_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on testing set is rating    0.602288\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# test on testing set\n",
    "y_test = data_test[\"rating\"]\n",
    "y_pred_test = [predict(i) for i in data_test[\"user_id\"]]\n",
    "MSE_test = MSE(y_pred_test, y_test)\n",
    "print(\"MSE on testing set is\", str(MSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity prediction\n",
    "using  similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allRatings = []\n",
    "# userRatings = defaultdict(list)\n",
    "\n",
    "# for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "#   r = int(r)\n",
    "#   allRatings.append(r)\n",
    "#   userRatings[user].append(r)\n",
    "\n",
    "# globalAverage = sum(allRatings) / len(allRatings)\n",
    "# userAverage = {}\n",
    "# for u in userRatings:\n",
    "#   userAverage[u] = sum(userRatings[u]) / len(userRatings[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f042946d8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"P/N\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# build text cleaning funcion\n",
    "punct = string.punctuation\n",
    "stemmer = PorterStemmer()\n",
    "stop = set(stopwords.words(\"english\"))  # set of stopwords\n",
    "\n",
    "\n",
    "def cleanText(text):\n",
    "    t = text.lower()  # lowercase string\n",
    "    t = [c for c in t if not (c in punct)]  # non-punct characters\n",
    "    t = [c for c in t if not (c.isdigit())]\n",
    "    t = \"\".join(t)  # convert back to string\n",
    "    t = t.strip().split()  # tokenizes\n",
    "    words = [c for c in t if not (c in stop)]  # remove stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the 10 most-frequently-occurring unigrams along with their number of occurrences in the corpus\n",
    "# build unigramCount dictionary\n",
    "unigramCount = defaultdict(int)\n",
    "for d in data_train[\"review_text\"]:\n",
    "    words = cleanText(d)\n",
    "    for w in words:\n",
    "        unigramCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramCounts = [(unigramCount[w], w) for w in unigramCount]\n",
    "unigramCounts.sort()\n",
    "unigramCounts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for unigram.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [(204844, 'dress'),\n",
       "  (74597, 'fit'),\n",
       "  (62852, 'size'),\n",
       "  (54231, 'would'),\n",
       "  (50345, 'wear'),\n",
       "  (46439, 'wore'),\n",
       "  (44208, 'great'),\n",
       "  (44073, 'little'),\n",
       "  (41767, 'perfect'),\n",
       "  (38196, 'comfortable')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 words for unigram.\"), unigramCounts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.2. bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the 10 most-frequently-occurring bigrams along with their number of occurrences in the corpus\n",
    "# build bigramCount dictionary\n",
    "\n",
    "bigramCount = defaultdict(int)\n",
    "for d in data_train[\"review_text\"]:\n",
    "    words = cleanText(d)\n",
    "    for w in range(len(words) - 1):\n",
    "        bigram = words[w] + \" \" + words[w + 1]\n",
    "        bigramCount[bigram] += 1\n",
    "\n",
    "bigramCounts = [(bigramCount[w], w) for w in bigramCount]\n",
    "bigramCounts.sort()\n",
    "bigramCounts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for bigram.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [(13777, 'true size'),\n",
       "  (12487, 'many compliments'),\n",
       "  (11052, 'loved dress'),\n",
       "  (9613, 'dress fit'),\n",
       "  (9086, 'fit perfectly'),\n",
       "  (7054, 'would definitely'),\n",
       "  (6313, 'received many'),\n",
       "  (6055, 'wore dress'),\n",
       "  (5494, 'dress perfect'),\n",
       "  (5466, 'great dress')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 words for bigram.\"), bigramCounts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 2000 unigrams to train a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 1000 unigrams to train a regressor\n",
    "uniwords = [x[1] for x in unigramCounts[:2000]]\n",
    "uniwordId = dict(zip(uniwords, range(len(uniwords))))\n",
    "uniwordSet = set(uniwords)\n",
    "\n",
    "\n",
    "def feature_uni(datum):\n",
    "    feat = [0] * len(uniwordSet)\n",
    "    words = cleanText(datum)\n",
    "    for w in words:\n",
    "        if not (w in uniwordSet):\n",
    "            continue\n",
    "        feat[uniwordId[w]] += 1\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_uni(d) for d in data_train[\"review_text\"]]\n",
    "y_train = [d for d in data_train[\"rating\"]]\n",
    "X_valid = [feature_uni(d) for d in data_valid[\"review_text\"]]\n",
    "y_valid = [d for d in data_valid[\"rating\"]]\n",
    "X_test = [feature_uni(d) for d in data_test[\"review_text\"]]\n",
    "y_test = [d for d in data_test[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train regression model\n",
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.Ridge(2.0, fit_intercept=False)  # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02629944,  0.03034688,  0.01027751, -0.03141446, -0.02492395,\n",
       "        0.04781551,  0.08056337,  0.03427384,  0.14210758,  0.0980875 ,\n",
       "        0.12559584,  0.1195039 , -0.04282727,  0.00641369,  0.00547121,\n",
       "        0.00032832,  0.05089087,  0.05811224, -0.06673966, -0.02427626])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive Coefficient  [(0.171619349540983, 'buying'), (0.17956885248039733, 'nicole'), (0.17974411851324626, 'glove'), (0.18315833765492484, 'flatters'), (0.19171917352113724, 'adore'), (0.19247067246109556, 'perfection'), (0.1936580392940384, 'dream'), (0.20662049199311994, 'movie'), (0.23952263966875728, 'happier'), (4.4396186177602575, 'constant_feat')]\n",
      "Top 10 negative coefficient  [(-0.7750650090470079, 'disappointing'), (-0.5770152143900348, 'unflattering'), (-0.36766105290605716, 'unable'), (-0.36029858990415753, 'frumpy'), (-0.35712876231527463, 'disappointed'), (-0.3570379010055114, 'odd'), (-0.3478392448445271, 'cheap'), (-0.34737844869288337, 'awkwardly'), (-0.3468383231176177, 'strange'), (-0.34456867352735676, 'bulky')]\n"
     ]
    }
   ],
   "source": [
    "weights = list(zip(theta, uniwords + [\"constant_feat\"]))\n",
    "weights.sort()\n",
    "print(\"Top 10 positive Coefficient \", weights[-10:])\n",
    "print(\"Top 10 negative coefficient \", weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on validation set and testing set\n",
    "y_pred_valid = clf.predict(X_valid)\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 0.38447959854428265\n",
      "MSE on validation set: 0.3613198470884725\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE for unigrams regressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE on validation set:\", mean_squared_error(y_valid, y_pred_valid))\n",
    "print(\"MSE on testing set:\", mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the 1000 bigrams to train a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 1000 bigrams to train a regressor\n",
    "biwords = [x[1] for x in bigramCounts[:2000]]\n",
    "biwordId = dict(zip(biwords, range(len(biwords))))\n",
    "biwordSet = set(biwords)\n",
    "\n",
    "\n",
    "def feature_bi(datum):\n",
    "    feat = [0] * len(biwordSet)\n",
    "    words = cleanText(datum)\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = words[i] + \" \" + words[i + 1]\n",
    "        if bigram in biwords:\n",
    "            feat[biwordId[bigram]] += 1\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_bi(d) for d in data_train[\"review_text\"]]\n",
    "y_train = [d for d in data_train[\"rating\"]]\n",
    "X_valid = [feature_bi(d) for d in data_valid[\"review_text\"]]\n",
    "y_valid = [d for d in data_valid[\"rating\"]]\n",
    "X_test = [feature_bi(d) for d in data_test[\"review_text\"]]\n",
    "y_test = [d for d in data_test[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False)  # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09440102,  0.18640503,  0.12545135, -0.02535028,  0.19624789,\n",
       "       -0.01453997, -0.04011656,  0.0553807 ,  0.09942871,  0.051852  ,\n",
       "        0.12848685, -0.07776247,  0.01601269, -0.02314007,  0.04580174,\n",
       "        0.19965921,  0.20557228, -0.04552579,  0.22480352, -0.0381269 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive Coefficient  [(0.2257736257475605, 'loved much'), (0.22662000347304684, 'want buy'), (0.2336798109342331, 'like million'), (0.23794805613231781, 'fits perfectly'), (0.23879455637804056, 'like princess'), (0.24258767973148854, 'wish owned'), (0.2606347820470421, 'say enough'), (0.2909078665944556, 'like dream'), (0.31645701834340617, 'wanted keep'), (4.4465064025062935, 'constant_feat')]\n",
      "Top 10 negative coefficient  [(-0.7416800999840821, 'unable wear'), (-0.7223609048190053, 'without wearing'), (-0.7103472524314987, 'wouldnt rent'), (-0.671610200650331, 'wasnt flattering'), (-0.549616788207872, 'way short'), (-0.5318084862915318, 'couldnt even'), (-0.5288910685092774, 'didnt work'), (-0.5277394190576751, 'end wearing'), (-0.515269552202804, 'gave stars'), (-0.4966872483622018, 'could barely')]\n"
     ]
    }
   ],
   "source": [
    "weights = list(zip(theta, biwords + [\"constant_feat\"]))\n",
    "weights.sort()\n",
    "print(\"Top 10 positive Coefficient \", weights[-10:])\n",
    "print(\"Top 10 negative coefficient \", weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on validation set and testing set\n",
    "y_pred_valid = clf.predict(X_valid)\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 0.40698806056531217\n",
      "MSE on validation set: 0.3892037389634508\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE for bigrams regressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE on validation set:\", mean_squared_error(y_valid, y_pred_valid))\n",
    "print(\"MSE on testing set:\", mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the 1000 unigrams and bigrams to train a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine unigrams and bigrams\n",
    "mergeCount = unigramCounts + bigramCounts\n",
    "mergeCount.sort()\n",
    "mergeCount.reverse()\n",
    "grams = [x[1] for x in mergeCount[:2000]]\n",
    "gramID = dict(zip(grams, range(len(grams))))\n",
    "gramSet = set(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mer(datum):\n",
    "    feat = [0] * len(gramSet)\n",
    "    text = cleanText(datum)\n",
    "    bg = list(bigrams(text)) # all bigrams in text\n",
    "    for w in text + bg:\n",
    "        if w in gramSet:\n",
    "            feat[gramID[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_mer(d) for d in data_train[\"review_text\"]]\n",
    "y_train = [d for d in data_train[\"rating\"]]\n",
    "X_valid = [feature_mer(d) for d in data_valid[\"review_text\"]]\n",
    "y_valid = [d for d in data_valid[\"rating\"]]\n",
    "X_test = [feature_mer(d) for d in data_test[\"review_text\"]]\n",
    "y_test = [d for d in data_test[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(0.05, fit_intercept=False)  # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02608017,  0.02984921,  0.00943985, -0.03253671, -0.02499468,\n",
       "        0.05010504,  0.08144777,  0.03229155,  0.14493623,  0.10016506,\n",
       "        0.13220991,  0.11999883, -0.04187307,  0.00454936,  0.00491307,\n",
       "       -0.00134528,  0.05132664,  0.06120607, -0.0718312 , -0.02708323])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive Coefficient  [(0.16121282805028816, 'deal'), (0.16191371058230716, 'heartbeat'), (0.16328434332329486, 'holds'), (0.17314485054931322, 'buying'), (0.175122374441242, 'incredible'), (0.1764951679980558, 'princess'), (0.18390212555850283, 'glove'), (0.1976967621532812, 'dream'), (0.24385331527923879, 'happier'), (4.436817089192557, 'constant_feat')]\n",
      "Top 10 negative coefficient  [(-0.5870434504405685, 'unflattering'), (-0.3879822541865254, 'unable'), (-0.3739239752013788, 'disappointed'), (-0.3667054335084634, 'odd'), (-0.36529467680870736, 'strange'), (-0.35514869356708845, 'cheap'), (-0.35236965653282926, 'awkward'), (-0.35103662323635587, 'unfortunately'), (-0.3374737853655394, 'bulky'), (-0.33323016019839613, 'returned')]\n"
     ]
    }
   ],
   "source": [
    "weights = list(zip(theta, grams + [\"constant_feat\"]))\n",
    "weights.sort()\n",
    "print(\"Top 10 positive Coefficient \", weights[-10:])\n",
    "print(\"Top 10 negative coefficient \", weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict on validation set and testing set\n",
    "y_pred_valid = clf.predict(X_valid)\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 0.38856938953442993\n",
      "MSE on validation set: 0.3648699201505146\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE for unigrams and bigrams regressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE on validation set:\", mean_squared_error(y_valid, y_pred_valid))\n",
    "print(\"MSE on testing set:\", mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE on validation set: 0.37746878280012636\n",
    "MSE on validation set: 0.39209168978072867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with tf-idf\n",
    "Firstly we extract every comment from data. Then calculate tf â idf for every segment in comment. After that, we use the logistic regression to assgin weight(coefficient) to words. The larger the coefficient, the better the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "for d in data_train['review_text']:   \n",
    "    words = cleanText(d)\n",
    "    for word in words:\n",
    "        wordCount[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function tfidf\n",
    "from math import log\n",
    "def tf(w,d):\n",
    "    count = 0\n",
    "    text = cleanText(d)\n",
    "    for t in text:\n",
    "        if t == w:\n",
    "            count += 1\n",
    "    return count/len(text)\n",
    "\n",
    "def idf(w,dataset):\n",
    "    return log(len(dataset) / wordCount[w]*1.0, 10)\n",
    "\n",
    "def tfidf(w,d,dataset):\n",
    "    return tf(w,d) * idf(w,dataset)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regression model with tfidf on unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 1000 unigrams to train a regressor\n",
    "uniwords = [x[1] for x in unigramCounts[:1000]]\n",
    "uniwordId = dict(zip(uniwords, range(len(uniwords))))\n",
    "uniwordSet = set(uniwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "def feature_tfidf_uni(datum,dataset):\n",
    "    feat = [0]*len(uniwordSet)\n",
    "    text = cleanText(datum)\n",
    "    for w in text:\n",
    "        if w in uniwordSet:\n",
    "            feat[uniwordId[w]] = tfidf(w,datum,dataset)\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_tfidf_uni(d,data_train) for d in data_train['review_text']]\n",
    "y_train = [d for d in data_train[\"rating\"]]\n",
    "X_valid = [feature_tfidf_uni(d,data_valid) for d in data_valid['review_text']]\n",
    "y_valid = [d for d in data_valid[\"rating\"]]\n",
    "X_test = [feature_tfidf_uni(d,data_test) for d in data_test['review_text']]\n",
    "y_test = [d for d in data_test[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1, fit_intercept=False)  # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.20007834,  0.15306657,  0.43383712, -1.66589775, -1.39375688,\n",
       "        2.52257482,  2.02717433,  2.09942877,  2.71430203,  1.43278419,\n",
       "        2.51535421,  1.8951484 , -1.90969393,  0.57134874,  0.52819009,\n",
       "        0.64319079,  1.74354214,  2.72208036, -2.82347704, -0.66407166])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive Coefficient  [(2.099428770211516, 'little'), (2.107478356441701, 'nervous'), (2.2000783383951505, 'dress'), (2.4392897275003604, 'glove'), (2.5153542132586435, 'compliments'), (2.5225748236190912, 'wore'), (2.7143020326307563, 'perfect'), (2.7220803614821, 'bra'), (2.8482866689187465, 'worried'), (4.416813328340744, 'constant_feat')]\n",
      "Top 10 negative coefficient  [(-4.814897000221017, 'unfortunately'), (-4.568198223141438, 'disappointed'), (-3.7437326248659515, 'unflattering'), (-3.6722893456438928, 'awkward'), (-3.64893690709843, 'excited'), (-3.4393210262893135, 'odd'), (-3.071351735108077, 'weird'), (-3.046249178070285, 'however'), (-2.846227354596591, 'returned'), (-2.823477037508764, 'didnt')]\n"
     ]
    }
   ],
   "source": [
    "weights = list(zip(theta, uniwords + [\"constant_feat\"]))\n",
    "weights.sort()\n",
    "print(\"Top 10 positive Coefficient \", weights[-10:])\n",
    "print(\"Top 10 negative coefficient \", weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on validation set and testing set\n",
    "y_pred_valid = clf.predict(X_valid)\n",
    "y_pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation set: 0.5612942964050983\n",
      "MSE on validation set: 0.5467620872967909\n"
     ]
    }
   ],
   "source": [
    "# calculate MSE for unigrams and bigrams regressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE on validation set:\", mean_squared_error(y_valid, y_pred_valid))\n",
    "print(\"MSE on testing set:\", mean_squared_error(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### regression model with tfidf on bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 1000 bigrams to train a regressor\n",
    "biwords = [x[1] for x in bigramCounts[:1000]]\n",
    "biwordId = dict(zip(biwords, range(len(biwords))))\n",
    "biwordSet = set(biwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tfidf\n",
    "def tf_bi(w,d):\n",
    "    count = 0\n",
    "    text = cleanText(d)\n",
    "    for i in range(len(text)-1):\n",
    "        bigram = text[i] + \" \" + text[i+1]\n",
    "        if bigram == w:\n",
    "            count += 1\n",
    "    return count/int(len(text)/2)\n",
    "\n",
    "def idf_bi(w,dataset):\n",
    "    return log(len(dataset) / wordCount[w]*1.0, 10)\n",
    "\n",
    "def tfidf(w,d,dataset):\n",
    "    return tf(w,d) * idf(w,dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "def feature_tfidf_bi(datum,dataset):\n",
    "    feat = [0]*len(biwordSet)\n",
    "    text = cleanText(datum)\n",
    "    for i in range(len(text)-1):\n",
    "        bi = text[i] + \" \" + text[i+1]\n",
    "        if bi in biwords:\n",
    "            feat[biwordId[w]] = tfidf(bi,datum,dataset)\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_tfidf_bi(d,data_train) for d in data_train['review_text']]\n",
    "y_train = [d for d in data_train[\"rating\"]]\n",
    "X_valid = [feature_tfidf_bi(d,data_valid) for d in data_valid['review_text']]\n",
    "y_valid = [d for d in data_valid[\"rating\"]]\n",
    "X_test = [feature_tfidf_bi(d,data_test) for d in data_test['review_text']]\n",
    "y_test = [d for d in data_test[\"rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1, fit_intercept=False)  # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to dictionary\n",
    "train_dict = data_train.to_dict(orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
